{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost,an activation function is any function that take one number and return another number.\n",
    "\n",
    "Constraint 1: the function must be continuous and infinite in domain\n",
    "\n",
    "Constrain 2: good activation functions ar 'monotonic',never changing direction\n",
    "\n",
    "Constrain 3: good activation functions are nonlinear(i.e., they squiggle or turn)\n",
    "    Relu function is non-linear activation function\n",
    "    \n",
    "Constrain 4: good activation functions(and their derivatives)should be efficiently computable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Activation Functions\n",
    " \n",
    " 1. Sigmoid function: Sigmoid is great because it smoothly squishes the infinite amount of input to an output between 0 and 1. In mnay circumstances, this let's you interpret the output of any individual neuron as a 'probability'. Thus, people use this non-linearity both in hidden layers and output layers.\n",
    " \n",
    " 2. Tanh: better than Sigmoid for Hidden. Remember how we wanted to model'selective correlation?' Well,Sigmoid gives you 'varying degrees of positive correlation.' That's nice. Tanh is the same as sigmoid except it is between -1 and 1. This means it can also throw in some negative correlation. While not that useful for output layers(unless the data you're predicting goes between -1 and 1),this aspect of negative correlation is very powerful for hidden layers and on many problems, tanh will outperform sigmoid in hidden layers.\n",
    "\n",
    "3. Softmax:e^x\n",
    "    sharpness of attenuation\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 majoy types of output layer\n",
    "\n",
    "1. Predicting raw data values(no activation function)\n",
    "    If I were training a network to do this prediction, I would very likely just train the network without an activation function on the output at all.\n",
    "\n",
    "2. Predict unrelated yes/no probabilities(sigmoid)\n",
    "\n",
    "3. Predict 'which one' probabilities(softmax)\n",
    "\n",
    "    Softmax is kind of Multi Class Sigmoid, but if you see the function of Softmax, the sum of all softmax units are supposed to be 1. In sigmoid itâ€™s not really necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Function            Forward Prop                   Back Prop Delta\n",
    "relu               one_and_zeros=(input>0)        mask=output>0\n",
    "                   output=input*one_and_zeros     deriv=output*mask\n",
    "\n",
    "sigmoid            output=1/(1+np.exp(-input))    deriv=output*(1-output)\n",
    "\n",
    "tanh               output=np.tanh(input)          deriv=1-(output**2)\n",
    "\n",
    "softmax            temp=np.exp(input)             temp=(output-true)\n",
    "                   output/=np.sum(temp)           output=temp/len(true)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Function: For the backpropagation process in a neural network, it means that your errors will be squeezed by (at least) a quarter at each layer. Therefore, deeper your network is, more knowledge from the data will be \"lost\". Some \"big\" errors we get from the output layer might not be able to affect the synapses weight of a neuron in a relatively shallow layer much (\"shallow\" means it's close to the input layer).\n",
    "\n",
    "Due to this, sigmoids have fallen out of favor as activations on hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activations are the simplest non-linear activation function you can use, obviously. When you get the input is positive, the derivative is just 1, so there isn't the squeezing effect you meet on backpropagated errors from the sigmoid function. Research has shown that ReLUs result in much faster training for large networks. Most frameworks like TensorFlow and TFLearn make it simple to use ReLUs on the the hidden layers, so you won't need to implement them yourself.\n",
    "\n",
    "However, such a simple solution is not always a perfect solution. From Andrej Karpathy's CS231n course:\n",
    "\n",
    "Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1 (check it on the figure above).\n",
    "\n",
    "The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
