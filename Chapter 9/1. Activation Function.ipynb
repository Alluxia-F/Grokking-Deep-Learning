{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost,an activation function is any function that take one number and return another number.\n",
    "\n",
    "Constraint 1: the function must be continuous and infinite in domain\n",
    "\n",
    "Constrain 2: good activation functions ar 'monotonic',never changing direction\n",
    "\n",
    "Constrain 3: good activation functions are nonlinear(i.e., they squiggle or turn)\n",
    "    Relu function is non-linear activation function\n",
    "    \n",
    "Constrain 4: good activation functions(and their derivatives)should be efficiently computable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Activation Functions\n",
    " \n",
    " 1. Sigmoid function: Sigmoid is great because it smoothly squishes the infinite amount of input to an output between 0 and 1. In mnay circumstances, this let's you interpret the output of any individual neuron as a 'probability'. Thus, people use this non-linearity both in hidden layers and output layers.\n",
    " \n",
    " 2. Tanh: better than Sigmoid for Hidden. Remember how we wanted to model'selective correlation?' Well,Sigmoid gives you 'varying degrees of positive correlation.' That's nice. Tanh is the same as sigmoid except it is between -1 and 1. This means it can also throw in some negative correlation. While not that useful for output layers(unless the data you're predicting goes between -1 and 1),this aspect of negative correlation is very powerful for hidden layers and on many problems, tanh will outperform sigmoid in hidden layers.\n",
    "\n",
    "3. Softmax:e^x\n",
    "    sharpness of attenuation\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 majoy types of output layer\n",
    "\n",
    "1. Predicting raw data values(no activation function)\n",
    "    If I were training a network to do this prediction, I would very likely just train the network without an activation function on the output at all.\n",
    "\n",
    "2. Predict unrelated yes/no probabilities(sigmoid)\n",
    "\n",
    "3. Predict 'which one' probabilities(softmax)\n",
    "\n",
    "    Softmax is kind of Multi Class Sigmoid, but if you see the function of Softmax, the sum of all softmax units are supposed to be 1. In sigmoid itâ€™s not really necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Function            Forward Prop                   Back Prop Delta\n",
    "relu               one_and_zeros=(input>0)        mask=output>0\n",
    "                   output=input*one_and_zeros     deriv=output*mask\n",
    "\n",
    "sigmoid            output=1/(1+np.exp(-input))    deriv=output*(1-output)\n",
    "\n",
    "tanh               output=np.tanh(input)          deriv=1-(output**2)\n",
    "\n",
    "softmax            temp=np.exp(input)             temp=(output-true)\n",
    "                   output/=np.sum(temp)           output=temp/len(true)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
